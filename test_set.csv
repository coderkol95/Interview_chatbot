Unique_ID,Topic,Question,Actual_answer,Answer_variation,Expected_score,TFIDF,CountVec,all_roberta,all_MiniLM,bert
1,SVM,What is C in SVM?,A model hyperparameter which is the regularisation parameter for the error term,Lagrange multiplier,0,0,0,242.73,200.474,502.908
2,SVM,What is C in SVM?,A model hyperparameter which is the regularisation parameter for the error term,Constant term,0,0.348,0.345,193.176,236.757,314.448
3,SVM,What is C in SVM?,A model hyperparameter which is the regularisation parameter for the error term,Regularisation parameter  ,400,0.862,0.81,773.696,675.293,562.921
4,SVM,What is C in SVM?,A model hyperparameter which is the regularisation parameter for the error term,Regularisation constant,300,0.502,0.454,620.512,465.998,396.58
5,SVM,What is C in SVM?,A model hyperparameter which is the regularisation parameter for the error term,Hyperparameter,200,0.976,0.959,619.576,543.677,595.475
6,SVM,What is C in SVM?,A model hyperparameter which is the regularisation parameter for the error term,model hyperparameter which is the regularisation term,700,0.968,0.937,927.358,886.659,779.168
7,SVM,What is C in SVM?,A model hyperparameter which is the regularisation parameter for the error term,model hyperparameter which is the regularisation parameter,900,0.976,0.958,922.409,887.2,871.119
8,STS,Define Simple random sampling?,"It means taking a small, random portion of the entire population to represent the entire data set, where each member has an equal probability of being chosen",a technique where each point has an equal probability of getting selected,300,0.215,0.229,638.958,448.584,756.417
9,STS,Define Simple random sampling?,"It means taking a small, random portion of the entire population to represent the entire data set, where each member has an equal probability of being chosen",taking small random portions of the entire population,400,0.954,0.903,720.303,569.574,674.862
10,STS,Define Simple random sampling?,"It means taking a small, random portion of the entire population to represent the entire data set, where each member has an equal probability of being chosen",randomly selecting each point from the population where each datapoint has the same probability of getting selected,900,0.181,0.224,547.743,506.024,853.805
11,STS,Define Simple random sampling?,"It means taking a small, random portion of the entire population to represent the entire data set, where each member has an equal probability of being chosen",each point can get randomly selected,500,0,0,526.649,430.356,612.612
12,STS,Define Simple random sampling?,"It means taking a small, random portion of the entire population to represent the entire data set, where each member has an equal probability of being chosen",dividing the population and then selecting at particular interval starting from a random seed,0,0.289,0.324,535.101,389.666,629.157
13,STS,Define Simple random sampling?,"It means taking a small, random portion of the entire population to represent the entire data set, where each member has an equal probability of being chosen",small random portions of population,200,0.742,0.639,626.985,523.591,459.865
14,STS,Define Simple random sampling?,"It means taking a small, random portion of the entire population to represent the entire data set, where each member has an equal probability of being chosen",machine learning technique to take points,0,0,0,213.751,132.684,337.909
15,STS,What is central limit theorem?,The sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger no matter what the shape of the population distribution.,probabilty distribution of sample means becomes similar to normal distribution as the sample size gets larger regardless of the distribution of the data points,900,0.859,0.8,790.494,680.007,880.337
16,STS,What is central limit theorem?,The sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger no matter what the shape of the population distribution.,sampling distribution of sample means becomes similar to normal distribution,700,0.678,0.651,734.621,643.641,755.787
17,STS,What is central limit theorem?,The sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger no matter what the shape of the population distribution.,mean of sample means becomes population mean,500,0.244,0.277,569.216,595.09,661.485
18,STS,What is central limit theorem?,The sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger no matter what the shape of the population distribution.,mean of sample means becomes,300,0.234,0.274,562.398,482.498,542.966
19,STS,What is central limit theorem?,The sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger no matter what the shape of the population distribution.,distribution of sample means is population mean,200,0.574,0.571,680.752,665.111,658.172
20,STS,What is central limit theorem?,The sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger no matter what the shape of the population distribution.,limiting the sample means becomes the population mean,0,0.28,0.321,646.035,602.796,541.102
21,STS,What is central limit theorem?,The sampling distribution of the sample mean approaches a normal distribution as the sample size gets larger no matter what the shape of the population distribution.,distribution of sample data points becomes the population mean with increasing the no. of samples,0,0.43,0.448,742.852,636.244,617.121
22,STS,Why is hypothesis testing done?,To determine the probability of the null hypothesis being true. This will enable us to accept the alternate hypothesis if the p value is very less,To find the probability of the null hypothesis being true so that we can accept the alternate hypothesis on the basis of the p value,900,0.708,0.72,808.771,816.877,824.618
23,STS,Why is hypothesis testing done?,To determine the probability of the null hypothesis being true. This will enable us to accept the alternate hypothesis if the p value is very less,To reject the null hypothesis and accept the alternate hypothesis,700,0.459,0.477,665.996,648.294,514.358
24,STS,Why is hypothesis testing done?,To determine the probability of the null hypothesis being true. This will enable us to accept the alternate hypothesis if the p value is very less,To reject the null hypothesis,500,0.217,0.282,757.072,654.822,334.15
25,STS,Why is hypothesis testing done?,To determine the probability of the null hypothesis being true. This will enable us to accept the alternate hypothesis if the p value is very less,To prove the alternate hypothesis to be true,500,0.351,0.394,634.988,539.539,564.875
26,STS,Why is hypothesis testing done?,To determine the probability of the null hypothesis being true. This will enable us to accept the alternate hypothesis if the p value is very less,To prove a hypothesis to be true,200,0.377,0.419,474.704,485.877,426.969
27,STS,Why is hypothesis testing done?,To determine the probability of the null hypothesis being true. This will enable us to accept the alternate hypothesis if the p value is very less,To reach a conclusion,0,0,0,356.367,375.712,195.893
28,STS,Why is hypothesis testing done?,To determine the probability of the null hypothesis being true. This will enable us to accept the alternate hypothesis if the p value is very less,To prove our point,0,0,0,376.98,312.554,196.335
29,STS,Define Correlation?,The extent to which two variables are linearly related,The level and magnitude of linear relation between two variables,900,0.218,0.292,806.312,759.867,893.761
30,STS,Define Correlation?,The extent to which two variables are linearly related,Linear relation between two variables,700,0.226,0.297,841.861,751.054,859.663
31,STS,Define Correlation?,The extent to which two variables are linearly related,Relationship between different variables,500,0.286,0.371,681.444,570.945,488.9
32,STS,Define Correlation?,The extent to which two variables are linearly related,How different variables are related,300,0.732,0.736,653.058,602.097,482.737
33,STS,Define Correlation?,The extent to which two variables are linearly related,Relationship between variables,200,0.312,0.388,698.927,583.516,642.537
34,STS,Define Correlation?,The extent to which two variables are linearly related,Relation betwen data points,0,0,0,568.79,273.16,577.092
35,STS,Define Correlation?,The extent to which two variables are linearly related,Polynomial relationship between variables,0,0.271,0.363,539.425,491.737,500.59
36,STS,Explain quartiles?,Type of percentile which divides the number of data points into four parts or quarters on the basis of their values,Separating the number of data points into four parts on the basis of their values,900,0.809,0.843,630.771,572.522,877.585
37,STS,Explain quartiles?,Type of percentile which divides the number of data points into four parts or quarters on the basis of their values,Separating the number of data points into four parts,700,0.849,0.781,597.142,534.355,799.259
38,STS,Explain quartiles?,Type of percentile which divides the number of data points into four parts or quarters on the basis of their values,DIfferent parts of the distribution on the basis of their values,500,0.402,0.475,445.41,322.801,490.607
39,STS,Explain quartiles?,Type of percentile which divides the number of data points into four parts or quarters on the basis of their values,Four parts of the distribution,300,0.326,0.174,420.499,325.93,675.852
40,STS,Explain quartiles?,Type of percentile which divides the number of data points into four parts or quarters on the basis of their values,How the points are distributed into different slabs,200,0.231,0.262,372.312,211.825,447.794
41,STS,Explain quartiles?,Type of percentile which divides the number of data points into four parts or quarters on the basis of their values,Distribution of data points,0,0.293,0.285,401.946,363.129,410.663
42,STS,Define standard deviation?,A quantity expressing by how much the samples in a distribution differ from the mean value for the group,Difference between the samples of a distribution and the mean,900,0.635,0.557,673.687,647.123,811.974
43,STS,Define standard deviation?,A quantity expressing by how much the samples in a distribution differ from the mean value for the group,The distribution of the samples in respect to the mean,700,0.43,0.428,513.025,678.449,684.59
44,STS,Define standard deviation?,A quantity expressing by how much the samples in a distribution differ from the mean value for the group,How much the samples differ from the mean value,500,0.901,0.828,734.933,672.261,878.843
45,STS,Define standard deviation?,A quantity expressing by how much the samples in a distribution differ from the mean value for the group,Difference between the samples and mean,200,0.465,0.411,567.834,555.648,777.824
46,STS,Define standard deviation?,A quantity expressing by how much the samples in a distribution differ from the mean value for the group,Relation between the samples and mean,0,0.494,0.427,537.15,623.931,632.663
47,STS,How to reduce the impact of multicollinearity between features?,Compute VIF and remove the feature having higher VIF between each pair of correlated features,Calculate VIF and remove the feature with higher VIF between pairs of correlated features,900,0.944,0.927,974.395,961.198,907.559
48,STS,How to reduce the impact of multicollinearity between features?,Compute VIF and remove the feature having higher VIF between each pair of correlated features,Delete features with high VIF values between sets of correlated features,700,0.456,0.419,865.904,781.755,750.874
49,STS,How to reduce the impact of multicollinearity between features?,Compute VIF and remove the feature having higher VIF between each pair of correlated features,Remove features having higher VIF values,500,0.856,0.778,774.846,658.409,633.921
50,STS,How to reduce the impact of multicollinearity between features?,Compute VIF and remove the feature having higher VIF between each pair of correlated features,Remove features having high VIF values,200,0.537,0.509,748.575,622.943,628.154
51,STS,How to reduce the impact of multicollinearity between features?,Compute VIF and remove the feature having higher VIF between each pair of correlated features,Remove features having multicollinearity,0,0.487,0.436,601.466,609.63,537.527
52,STS,How to reduce the impact of multicollinearity between features?,Compute VIF and remove the feature having higher VIF between each pair of correlated features,Remove features that are collinear,0,0.306,0.302,568.449,568.304,377.987
53,STS,What is an observation called an outlier?,It is called an outlier when the observation differs significantly from other observations,A data point that is very different from other observations,900,0.197,0.193,650.117,485.998,740.043
54,STS,What is an observation called an outlier?,It is called an outlier when the observation differs significantly from other observations,When the observation is not like other observations,700,0.4,0.449,633.818,519.2,796.152
55,STS,What is an observation called an outlier?,It is called an outlier when the observation differs significantly from other observations,When the observation is very far from other observations,500,0.416,0.459,540.957,501.694,657.496
56,STS,What is an observation called an outlier?,It is called an outlier when the observation differs significantly from other observations,When the observation is away from the distribution,300,0.24,0.218,554.439,411.992,541.901
57,STS,What is an observation called an outlier?,It is called an outlier when the observation differs significantly from other observations,The observation is special,0,0.317,0.334,405.732,416.157,441.797
58,STS,What is an observation called an outlier?,It is called an outlier when the observation differs significantly from other observations,When the observation is unique,0,0.317,0.334,443.687,388.047,572.783
59,KNN,"What is “K” in the KNN Algorithm? What is it used for, in classification?","K represents the number of neighbours you want to select to predict the class of a given item, which is coming as an unseen data for the model.",The no. of neighbours selected to predict the class of a given item which is not previously seen by the model,900,0.919,0.863,668.397,622.966,641.976
60,KNN,"What is “K” in the KNN Algorithm? What is it used for, in classification?","K represents the number of neighbours you want to select to predict the class of a given item, which is coming as an unseen data for the model.",The no. of neighbours selected to predict the class of a given item,700,0.969,0.953,631.656,602.008,630.115
61,KNN,"What is “K” in the KNN Algorithm? What is it used for, in classification?","K represents the number of neighbours you want to select to predict the class of a given item, which is coming as an unseen data for the model.",The number of neighbours to be taken by the model,500,0.506,0.424,597.551,551.895,622.78
62,KNN,"What is “K” in the KNN Algorithm? What is it used for, in classification?","K represents the number of neighbours you want to select to predict the class of a given item, which is coming as an unseen data for the model.",Modal class is the prediction,300,0.27,0.28,311.542,288.466,163.112
63,KNN,"What is “K” in the KNN Algorithm? What is it used for, in classification?","K represents the number of neighbours you want to select to predict the class of a given item, which is coming as an unseen data for the model.",Model hyperparameter,200,0.076,0.119,343.593,299.62,91.498
64,KNN,"What is “K” in the KNN Algorithm? What is it used for, in classification?","K represents the number of neighbours you want to select to predict the class of a given item, which is coming as an unseen data for the model.",Used to predict unseen data,0,0.347,0.326,497.261,487.671,372.351
65,KNN,"What is “K” in the KNN Algorithm? What is it used for, in classification?","K represents the number of neighbours you want to select to predict the class of a given item, which is coming as an unseen data for the model.",Average of k neighbors is the prediction of the model,0,0.058,0.102,563.975,685.374,753.818
66,KNN,Why Knn is a non-parametric algorithm?,It does not make any assumption of underlying data distribution,It does not assume any distribution of the underlying data,900,0.785,0.737,902.596,919.893,930.994
67,KNN,Why Knn is a non-parametric algorithm?,It does not make any assumption of underlying data distribution,No assumption of underlying distribution,700,0.848,0.747,698.444,759.387,855.144
68,KNN,Why Knn is a non-parametric algorithm?,It does not make any assumption of underlying data distribution,No assumption about the data,500,0.477,0.459,689.969,732.506,734.713
69,KNN,Why Knn is a non-parametric algorithm?,It does not make any assumption of underlying data distribution,No information about the dataset,300,0,0,397.308,449.756,786.004
70,KNN,Why Knn is a non-parametric algorithm?,It does not make any assumption of underlying data distribution,It knows the distribution of the underlying data,0,0.778,0.675,704.73,582.588,221.745
71,KNN,What is kd-tree?,k dimensional trees are constructed in the feature space to narrow down the prospective space with a fixed leaf size,k dimensional trees are constructed in the feature space to reduce the prospective space with a fixed leaf size,900,0.97,0.965,975.092,973.003,913.513
72,KNN,What is kd-tree?,k dimensional trees are constructed in the feature space to narrow down the prospective space with a fixed leaf size,k dimensional trees are constructed in the feature space to reduce the search space,700,0.967,0.962,863.353,832.706,843.244
73,KNN,What is kd-tree?,k dimensional trees are constructed in the feature space to narrow down the prospective space with a fixed leaf size,k dimensional trees are constructed to reduce the search space,500,0.891,0.876,756.825,767.416,832.189
74,KNN,What is kd-tree?,k dimensional trees are constructed in the feature space to narrow down the prospective space with a fixed leaf size,reducing the search space by constructing trees,300,0.563,0.58,587.649,601.2,585.63
75,KNN,What is kd-tree?,k dimensional trees are constructed in the feature space to narrow down the prospective space with a fixed leaf size,Hyperparameter to make calculations faster,200,0,0,62.803,47.062,156.833
76,KNN,What is kd-tree?,k dimensional trees are constructed in the feature space to narrow down the prospective space with a fixed leaf size,a technique to make decision trees using knn,0,0.118,0.129,383.203,467.453,507.963
77,KNN,What is kd-tree?,k dimensional trees are constructed in the feature space to narrow down the prospective space with a fixed leaf size,a hyperparameter to construct trees,0,0.209,0.227,504.032,517.508,571.303
78,KNN,When does KNN fail?,When there are extremely large no. of features,When the number of features is very large,900,0.5,0.571,941.586,872.219,527.624
79,KNN,When does KNN fail?,When there are extremely large no. of features,Very large number of features,900,0.47,0.549,833.703,828.761,392.768
80,KNN,When does KNN fail?,When there are extremely large no. of features,When there are a lot of features,800,0.449,0.51,845.903,826.947,378.741
81,KNN,When does KNN fail?,When there are extremely large no. of features,When the number of variables is very large,300,0.288,0.302,621.925,438.359,456.418
82,KNN,When does KNN fail?,When there are extremely large no. of features,When the data is very large,0,0.317,0.316,674.703,484.716,496.775
83,KNN,When does KNN fail?,When there are extremely large no. of features,When the number of samples is very large,0,0.31,0.341,596.994,367.212,434.904
84,Linear Regression,What do you mean by Efficiency of estimators ?,Coefficients are unbiased and have constant variance,it tells about precision of estimator,0,0,0,434.689,227.361,629.229
85,Linear Regression,What do you mean by Efficiency of estimators ?,Coefficients are unbiased and have constant variance,coefficients are unbiased,200,0.925,0.89,885.729,867.877,883.789
86,Linear Regression,What do you mean by Efficiency of estimators ?,Coefficients are unbiased and have constant variance,coefficients have constant variance,600,0.843,0.856,824.58,835.906,790.209
87,Linear Regression,What do you mean by Efficiency of estimators ?,Coefficients are unbiased and have constant variance,unbiased coefficients have constant variance,900,0.93,0.928,906.627,963.901,889.035
88,Linear Regression,What is Durbin Watson Test ?,It is test of autocorrelation in errors.,it is used to check normality of data,0,0,0,609.685,560.152,355.27
89,Linear Regression,What is Durbin Watson Test ?,It is test of autocorrelation in errors.,it is used to see if there is heteroscedasticity,0,0,0,670.231,601.635,404.058
90,Linear Regression,What is Durbin Watson Test ?,It is test of autocorrelation in errors.,it check for autocorrelation between predicted values,200,0.575,0.464,771.446,650.514,721.366
91,Linear Regression,What is Durbin Watson Test ?,It is test of autocorrelation in errors.,it checks for autocorrelation in residuals,900,0.632,0.567,795.798,695.221,781.254
92,Linear Regression,What is the use of Shapiro Wilk test in Linear regression ?,It is test to check if the probability distribution function of response / target variable follows normal distribution or not.,it is used to test the multicollinearity,0,0.126,0.126,537.924,465.13,381.654
93,Linear Regression,What is the use of Shapiro Wilk test in Linear regression ?,It is test to check if the probability distribution function of response / target variable follows normal distribution or not.,it is used to check if distribution of varibles is skewed or not ,200,0.266,0.314,716.427,554.055,422.169
94,Linear Regression,What is the use of Shapiro Wilk test in Linear regression ?,It is test to check if the probability distribution function of response / target variable follows normal distribution or not.,it is used to see if output variable has normal distribution or not,900,0.432,0.451,801.149,690.256,812.974
95,Linear Regression,What is assumption of Normality in linear regression ?,Residuals in fitted model are normally distributed,variables are not normally distributed,0,0.847,0.728,570.758,509.877,275.652
96,Linear Regression,What is assumption of Normality in linear regression ?,Residuals in fitted model are normally distributed,predictors are normally distributed,200,0.831,0.765,655.287,539.149,672.831
97,Linear Regression,What is assumption of Normality in linear regression ?,Residuals in fitted model are normally distributed,errors are normally distributed,900,0.841,0.818,545.681,548.623,530.733
98,SVM,What is a hyperplane in SVM,It is the decision boundary in a multidimensional space to separate different classes,decision boundary in higher dimensions to separate different classes,900,0.823,0.836,778.786,829.168,846.471
99,SVM,What is a hyperplane in SVM,It is the decision boundary in a multidimensional space to separate different classes,a plane that separates different classes,700,0.873,0.897,479.072,528.927,515.726
100,SVM,What is a hyperplane in SVM,It is the decision boundary in a multidimensional space to separate different classes,plane supported by support vectors,600,0,0,263.249,212.882,147.322
101,SVM,What is a hyperplane in SVM,It is the decision boundary in a multidimensional space to separate different classes,plane for classification,300,0,0,563.566,431.65,278.101
102,SVM,What is a hyperplane in SVM,It is the decision boundary in a multidimensional space to separate different classes,plane in multidimension space,200,0.527,0.537,337.668,469.839,214.767
103,SVM,What is a hyperplane in SVM,It is the decision boundary in a multidimensional space to separate different classes,support vector plane,0,0,0,447.045,296.953,190.712
104,SVM,"What term(s) is/are added to the objective function(using quadratic loss), due to soft margin?",The error term for misclassifications and the constraint term for the misclassifications,error and constraint term for miscalssifications will be added,900,0.87,0.835,606.114,648.344,781.585
105,SVM,"What term(s) is/are added to the objective function(using quadratic loss), due to soft margin?",The error term for misclassifications and the constraint term for the misclassifications,both error term and constraint term,700,0.866,0.848,645.72,625.698,781.392
106,SVM,"What term(s) is/are added to the objective function(using quadratic loss), due to soft margin?",The error term for misclassifications and the constraint term for the misclassifications,error term for miclassficiations,500,0.736,0.716,583.113,608.107,600.024
107,SVM,"What term(s) is/are added to the objective function(using quadratic loss), due to soft margin?",The error term for misclassifications and the constraint term for the misclassifications,constriant term for misclassifications,500,0.881,0.88,626.853,721.294,719.262
108,SVM,"What term(s) is/are added to the objective function(using quadratic loss), due to soft margin?",The error term for misclassifications and the constraint term for the misclassifications,quadratic loss term will be added to objective function using quadratic loss,0,0.133,0.145,465.668,257.796,361.049
109,SVM,"What term(s) is/are added to the objective function(using quadratic loss), due to soft margin?",The error term for misclassifications and the constraint term for the misclassifications,weight term of soft margin,0,0.842,0.822,314.678,253.986,284.994
110,Naive Bayes,What is alpha in Naive Bayes,Alpha is the smoothing parameter,a smooting parameter,900,0.759,0.757,662.471,341.506,409.161
111,Naive Bayes,What is alpha in Naive Bayes,Alpha is the smoothing parameter,a hyperparameter,700,0,0,399.897,299.983,223.122
112,Naive Bayes,What is alpha in Naive Bayes,Alpha is the smoothing parameter,alpha is model parameter,0,0.793,0.692,717.762,692.107,749.285
113,Naive Bayes,Why is Naive Bayes naive?,It makes the naive assumption that the features are independent,Assumption is that the features are independent of each other,900,0.778,0.763,846.766,867.755,846.765
114,Naive Bayes,Why is Naive Bayes naive?,It makes the naive assumption that the features are independent,because features are conditionally independent,700,0.324,0.403,841.777,837.165,784.305
115,Naive Bayes,Why is Naive Bayes naive?,It makes the naive assumption that the features are independent,assumtions made are naïve,0,0,0,373.906,402.39,623.296
116,Naive Bayes,Why is Naive Bayes naive?,It makes the naive assumption that the features are independent,a simple assumtion is made to make the calculations easier,400,0,0,367.741,254.146,285.06
117,Naive Bayes,Why is Naive Bayes naive?,It makes the naive assumption that the features are independent,it is based on bayes theorem,100,0,0,450.84,284.481,270.108
118,Naive Bayes,When do you use Laplace smoothing?,When the model encounters previously unseen data,laplace smoothing is used in naïve bayes when model encounters previously unseen data,900,0.972,0.968,549.141,575.24,602.459
119,Naive Bayes,When do you use Laplace smoothing?,When the model encounters previously unseen data,when model is fed with unknown data,800,0.353,0.451,762.486,715.986,714.004
120,Naive Bayes,When do you use Laplace smoothing?,When the model encounters previously unseen data,It is a smoothing technique,0,0,0,154.148,151.499,313.967
121,Naive Bayes,When do you use Laplace smoothing?,When the model encounters previously unseen data,Laplace smoothing is a smoothing technique that helps tackle the problem of zero probability,700,0,0,292.448,160.807,352.096
122,Naive Bayes,When do you use Laplace smoothing?,When the model encounters previously unseen data,When the model encounters previously seen data,0,0.978,0.949,875.665,907.76,698.054
123,Naive Bayes,When do you use Laplace smoothing?,When the model encounters previously unseen data,when variable gets an unknown value,900,0,0,354.682,329.598,486.755
124,Naive Bayes,When do you use Laplace smoothing?,When the model encounters previously unseen data,new data comes,600,0.139,0.252,370.664,490.419,567.026
125,Decision Tree,Why is decision tree called as Greedy algorithm?,"Because it only looks in the forward direction and once a split is made, it is not changed afterwards",Moves only in forward direction and decision is not changed afterwards,900,0.908,0.894,321.29,411.719,795.119
126,Decision Tree,Why is decision tree called as Greedy algorithm?,"Because it only looks in the forward direction and once a split is made, it is not changed afterwards",A decision once made is final,400,0,0,107.453,146,344.589
127,Decision Tree,Why is decision tree called as Greedy algorithm?,"Because it only looks in the forward direction and once a split is made, it is not changed afterwards",No changes made in the tree structure later on,600,0,0,312.426,226.096,459.409
128,Decision Tree,Why is decision tree called as Greedy algorithm?,"Because it only looks in the forward direction and once a split is made, it is not changed afterwards",Optimal decision is made at each step and is not revised afterwards,700,0,0,277.188,224.01,664.55
129,Decision Tree,Why is decision tree called as Greedy algorithm?,"Because it only looks in the forward direction and once a split is made, it is not changed afterwards",greedy is set as a hyperparameter for the algorithm,0,0,0,122.336,137.953,24.398
130,Decision Tree,Why is decision tree called as Greedy algorithm?,"Because it only looks in the forward direction and once a split is made, it is not changed afterwards",because it gives best results,0,0,0,294.456,361.919,281.041
131,Decision Tree,Why do we require Pruning in Decision Trees?,To reduce overfitting or to overcome problem of overfitting,To overcome the problem of overfitting,900,0.993,0.993,912.936,816.535,829.833
132,Decision Tree,Why do we require Pruning in Decision Trees?,To reduce overfitting or to overcome problem of overfitting,It reduces the size of decision trees by removing parts of the tree,600,0,0,336.318,334.214,426.831
133,Decision Tree,Why do we require Pruning in Decision Trees?,To reduce overfitting or to overcome problem of overfitting,Pruning is done to reduce the testing error,700,0.517,0.377,361.234,341.984,436.536
134,Decision Tree,Why do we require Pruning in Decision Trees?,To reduce overfitting or to overcome problem of overfitting,To make the model perform better,400,0,0,576.583,454.987,410.397
135,Decision Tree,Why do we require Pruning in Decision Trees?,To reduce overfitting or to overcome problem of overfitting,pruning means cutting short,200,0,0,113.857,277.073,314.168
136,Decision Tree,Why do we require Pruning in Decision Trees?,To reduce overfitting or to overcome problem of overfitting,pruning is needed in decision trees,0,0,0,295.959,359.689,328.81
137,Decision Tree,Why does decision tree overfit?,"When grown to full depth, they tend to learn the pattern of data and also noise and hence tend to overfit",Fully grown decision trees not only learn the patterns in the data but also the noise and so overfit,900,0.668,0.777,729.039,613.713,777.919
138,Decision Tree,Why does decision tree overfit?,"When grown to full depth, they tend to learn the pattern of data and also noise and hence tend to overfit",It not only learns the data pattern but also the noise,800,0.928,0.85,670.605,555.865,709.549
139,Decision Tree,Why does decision tree overfit?,"When grown to full depth, they tend to learn the pattern of data and also noise and hence tend to overfit",becaue they are low bias and high variance model,700,0.005,0.019,431.716,319.388,190.137
140,Decision Tree,Why does decision tree overfit?,"When grown to full depth, they tend to learn the pattern of data and also noise and hence tend to overfit",They go through each and every data point to make the decisions,600,0.45,0.696,176.63,323.618,511.814
141,Decision Tree,Why does decision tree overfit?,"When grown to full depth, they tend to learn the pattern of data and also noise and hence tend to overfit",becauese the residual or errors are least in decision tree,100,0.001,0,378.787,213.476,58.101
142,Decision Tree,Why does decision tree overfit?,"When grown to full depth, they tend to learn the pattern of data and also noise and hence tend to overfit",It is the best model,0,0.015,0.014,120.445,107.688,203.556
143,Kmeans,What is init++?,Random initialization of initial cluster centroids inversely proportional to distance,Random initialization of initial cluster centroids inversely proportional to distance,900,1,1,1000,1000,1000
144,Kmeans,What is init++?,Random initialization of initial cluster centroids inversely proportional to distance,Random initialization of initial cluster centroids,700,0.994,0.99,902.71,934.758,845.206
145,Kmeans,What is init++?,Random initialization of initial cluster centroids inversely proportional to distance,initial cluster centroids,200,0.98,0.974,752.612,801.774,634.797
146,Kmeans,What is init++?,Random initialization of initial cluster centroids inversely proportional to distance,for better clusters,300,0,0,353.803,290.006,434.592
147,Kmeans,Why Kmeans is effected by outliers?,It uses mean of the data points to find the cluster center,As it uses average of data to find cluster centroids,900,0.459,0.509,912.163,803.248,786.723
148,Kmeans,Why Kmeans is effected by outliers?,It uses mean of the data points to find the cluster center,Because outliers make the model less efficient as whole data is taken,500,0.124,0.186,338.573,222.235,178.228
149,Kmeans,Why Kmeans is effected by outliers?,It uses mean of the data points to find the cluster center,outliers are extreme point to they are bad,100,0,0,184.206,194.585,36.722
150,Kmeans,Why Kmeans is effected by outliers?,It uses mean of the data points to find the cluster center,kmeans takes mean of entire data,700,0.163,0.289,498.561,422.84,523.743
151,Kmeans,In Kmeans what does 'K' specify?,Number of clusters,nearest points,0,0,0,241.785,201.827,498.573
152,Kmeans,In Kmeans what does 'K' specify?,Number of clusters,how many clusters need to be formed,900,0.979,0.964,807.832,774.608,794.967
153,Kmeans,In Kmeans what does 'K' specify?,Number of clusters,it’s a hyperparameter,700,0,0,188.705,85.63,347.663
154,Kmeans,In Kmeans what does 'K' specify?,Number of clusters,model parameter,0,0,0,101.941,-7.666,328.987
